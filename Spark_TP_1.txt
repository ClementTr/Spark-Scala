## Question 1 ##
# Charger le fichier README.md (se trouvant dans le dossier Spark qui a été décompressé) dans un RDD
val filename = sc.textFile("README.md")


## Question 2 ##
# Afficher dans le terminal les 5 premières lignes du fichier
val filename = sc.textFile("README.md")
filename.take(5).foreach(println)


## Question 3 ##
# Faire un word count sur le README.md du dossier spark-2.2.0-hadoop2.6
# et afficher les résultats (mettre les résultats dans un dataFrame avec 
# une colonne “word” et une colonne “count”)
val textFile = sc.textFile("README.md") 
val counts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
val df_init = counts.toDF()
val newNames = Seq("word","count")
val df = df_init.toDF(newNames: _*)
df.show()


## Question 4 ##
# Afficher les résultats du word count sous forme de table, on veut les mots les plus fréquents en haut
val textFile = sc.textFile("README.md") 
val counts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
val df_init = counts.toDF()
val newNames = Seq("word","count")
val df_named = df_init.toDF(newNames: _*)
val df_ordered = df_named.orderBy(desc("count"))
df_ordered.show()


## Question 5 ##
# Un même mot peut être présent dans la table précédente avec des majuscules et minuscules (“The” et “the”). 
# Modifier le dataframe précédent pour  mettre les mots en lower case dans la colonne “word”.
val textFile = sc.textFile("README.md") 
val counts = textFile.flatMap(line => line.split(" ")).map(word => (word.toLowerCase, 1)).reduceByKey(_ + _)
val df_init = counts.toDF()
val newNames = Seq("word","count")
val df_named = df_init.toDF(newNames: _*)
val df_ordered = df_named.orderBy(desc("count"))
df_ordered.show()


